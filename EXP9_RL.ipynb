{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashwatverma81/Python_RL/blob/main/EXP9_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play BlackJack function\n",
        "The player is prompted to \"hit\" (take another card) or \"stand\" (keep the current hand) until their hand's total is less than 21."
      ],
      "metadata": {
        "id": "oa7GoMxx5go0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByIK-JyY1-KC",
        "outputId": "9596d791-2585-4ba4-f772-465cbc58eb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of Monte Carlo simulations: 2\n",
            "Do you want to 'hit' or 'stand'? hit\n",
            "Your hand: [5, 6, 4]\n",
            "Do you want to 'hit' or 'stand'? stand\n",
            "Your hand: [5, 6, 4]\n",
            "Dealer's hand: [9, 1, 4, 8]\n",
            "You win!\n",
            "Do you want to 'hit' or 'stand'? stand\n",
            "Your hand: [4, 7]\n",
            "Dealer's hand: [8, 7, 10]\n",
            "You win!\n",
            "\n",
            "Results after 2 simulations:\n",
            "Win percentage: 100.0%\n",
            "Loss percentage: 0.0%\n",
            "Tie percentage: 0.0%\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def get_card():\n",
        "    \"\"\"Get a random card value.\"\"\"\n",
        "    card = random.randint(1, 11)  # Ace can be 1 or 11\n",
        "    return card\n",
        "\n",
        "def play_blackjack():\n",
        "    \"\"\"Play one round of Blackjack.\"\"\"\n",
        "    player_hand = [get_card(), get_card()]\n",
        "    dealer_hand = [get_card(), get_card()]\n",
        "\n",
        "    while sum(player_hand) < 21:\n",
        "        action = input(\"Do you want to 'hit' or 'stand'? \").lower()\n",
        "\n",
        "        if action == 'hit':\n",
        "            player_hand.append(get_card())\n",
        "        elif action == 'stand':\n",
        "            break\n",
        "\n",
        "        print(f\"Your hand: {player_hand}\")\n",
        "\n",
        "    while sum(dealer_hand) < 17:\n",
        "        dealer_hand.append(get_card())\n",
        "\n",
        "    print(f\"Your hand: {player_hand}\")\n",
        "    print(f\"Dealer's hand: {dealer_hand}\")\n",
        "\n",
        "    if sum(player_hand) > 21:\n",
        "        print(\"Bust! You lose.\")\n",
        "        return -1\n",
        "    elif sum(dealer_hand) > 21 or sum(player_hand) > sum(dealer_hand):\n",
        "        print(\"You win!\")\n",
        "        return 1\n",
        "    elif sum(player_hand) == sum(dealer_hand):\n",
        "        print(\"It's a tie!\")\n",
        "        return 0\n",
        "    else:\n",
        "        print(\"You lose.\")\n",
        "        return -1\n",
        "\n",
        "def monte_carlo_simulation(num_simulations):\n",
        "    \"\"\"Run multiple simulations to estimate the expected outcome.\"\"\"\n",
        "    total_wins = 0\n",
        "    total_losses = 0\n",
        "    total_ties = 0\n",
        "\n",
        "    for _ in range(num_simulations):\n",
        "        result = play_blackjack()\n",
        "\n",
        "        if result == 1:\n",
        "            total_wins += 1\n",
        "        elif result == 0:\n",
        "            total_ties += 1\n",
        "        else:\n",
        "            total_losses += 1\n",
        "\n",
        "    win_percentage = (total_wins / num_simulations) * 100\n",
        "    loss_percentage = (total_losses / num_simulations) * 100\n",
        "    tie_percentage = (total_ties / num_simulations) * 100\n",
        "\n",
        "    print(f\"\\nResults after {num_simulations} simulations:\")\n",
        "    print(f\"Win percentage: {win_percentage}%\")\n",
        "    print(f\"Loss percentage: {loss_percentage}%\")\n",
        "    print(f\"Tie percentage: {tie_percentage}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_simulations = int(input(\"Enter the number of Monte Carlo simulations: \"))\n",
        "    monte_carlo_simulation(num_simulations)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MonteCarloFirstVisitAB:\n",
        "    def __init__(self, gamma=0.9):\n",
        "        self.states = ['A', 'B']\n",
        "        self.actions = [0, 1]  # For simplicity, assuming two actions\n",
        "        self.gamma = gamma\n",
        "        self.q_values = {(s, a): 0.0 for s in self.states for a in self.actions}\n",
        "        self.visit_counts = {(s, a): 0 for s in self.states for a in self.actions}\n",
        "\n",
        "    def choose_action(self, state, epsilon=0.1):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            return max(self.actions, key=lambda a: self.q_values[state, a])\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0  # Return (cumulative reward)\n",
        "        visited = set()  # Keep track of visited state-action pairs\n",
        "\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            sa_pair = (state, action)\n",
        "\n",
        "            # If the state-action pair is not visited in this episode, update Q-values\n",
        "            if sa_pair not in visited:\n",
        "                visited.add(sa_pair)\n",
        "                self.visit_counts[sa_pair] += 1\n",
        "                G = self.gamma * G + reward\n",
        "                self.q_values[sa_pair] += (1 / self.visit_counts[sa_pair]) * (G - self.q_values[sa_pair])\n",
        "\n",
        "    def run_episodes(self, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            state = np.random.choice(self.states)\n",
        "            episode_history = []\n",
        "\n",
        "            while state in self.states:\n",
        "                action = self.choose_action(state)\n",
        "                new_state, reward = self.take_action(state, action)\n",
        "                episode_history.append((state, action, reward))\n",
        "\n",
        "                if new_state is None:\n",
        "                    break  # Terminal state reached\n",
        "\n",
        "                state = new_state\n",
        "\n",
        "            self.update_q_values(episode_history)\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        # Implement your environment's dynamics here\n",
        "        # Return the new state and the reward\n",
        "        # For simplicity, let's assume a simple environment with states A and B\n",
        "        if state == 'A':\n",
        "            if action == 0:\n",
        "                return 'B', 1.0\n",
        "            else:\n",
        "                return 'A', 0.0\n",
        "        elif state == 'B':\n",
        "            if action == 0:\n",
        "                return 'A', -1.0\n",
        "            else:\n",
        "                return None, 2.0  # Terminal state\n",
        "\n",
        "# Example usage:\n",
        "num_episodes = 1000\n",
        "\n",
        "mc_first_visit_ab = MonteCarloFirstVisitAB()\n",
        "mc_first_visit_ab.run_episodes(num_episodes)\n",
        "\n",
        "# Print the learned Q-values for states A and B:\n",
        "print(\"Learned Q-values for state A:\")\n",
        "for action in mc_first_visit_ab.actions:\n",
        "    print(f\"Q(A, {action}): {mc_first_visit_ab.q_values[('A', action)]}\")\n",
        "\n",
        "print(\"Learned Q-values for state B:\")\n",
        "for action in mc_first_visit_ab.actions:\n",
        "    print(f\"Q(B, {action}): {mc_first_visit_ab.q_values[('B', action)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eUKzg3h-uk2",
        "outputId": "fbe546e1-5333-472a-dcd1-9a9fa902eb9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values for state A:\n",
            "Q(A, 0): 2.8\n",
            "Q(A, 1): 2.52\n",
            "Learned Q-values for state B:\n",
            "Q(B, 0): 1.512125\n",
            "Q(B, 1): 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MonteCarloEveryVisitAB:\n",
        "    def __init__(self, gamma=0.9):\n",
        "        self.states = ['A', 'B']\n",
        "        self.actions = [0, 1]  # For simplicity, assuming two actions\n",
        "        self.gamma = gamma\n",
        "        self.q_values = {(s, a): 0.0 for s in self.states for a in self.actions}\n",
        "        self.visit_counts = {(s, a): 0 for s in self.states for a in self.actions}\n",
        "\n",
        "    def choose_action(self, state, epsilon=0.1):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            return max(self.actions, key=lambda a: self.q_values[state, a])\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0  # Return (cumulative reward)\n",
        "\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            sa_pair = (state, action)\n",
        "\n",
        "            # Increment visit count for the state-action pair\n",
        "            self.visit_counts[sa_pair] += 1\n",
        "\n",
        "            # Update Q-value using incremental formula\n",
        "            G = self.gamma * G + reward\n",
        "            self.q_values[sa_pair] += (1 / self.visit_counts[sa_pair]) * (G - self.q_values[sa_pair])\n",
        "\n",
        "    def run_episodes(self, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            state = np.random.choice(self.states)\n",
        "            episode_history = []\n",
        "\n",
        "            while state in self.states:\n",
        "                action = self.choose_action(state)\n",
        "                new_state, reward = self.take_action(state, action)\n",
        "                episode_history.append((state, action, reward))\n",
        "\n",
        "                if new_state is None:\n",
        "                    break  # Terminal state reached\n",
        "\n",
        "                state = new_state\n",
        "\n",
        "            self.update_q_values(episode_history)\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        # Implement your environment's dynamics here\n",
        "        # Return the new state and the reward\n",
        "        # For simplicity, let's assume a simple environment with states A and B\n",
        "        if state == 'A':\n",
        "            if action == 0:\n",
        "                return 'B', 1.0\n",
        "            else:\n",
        "                return 'A', 0.0\n",
        "        elif state == 'B':\n",
        "            if action == 0:\n",
        "                return 'A', -1.0\n",
        "            else:\n",
        "                return None, 2.0  # Terminal state\n",
        "\n",
        "# Example usage:\n",
        "num_episodes = 1000\n",
        "\n",
        "mc_every_visit_ab = MonteCarloEveryVisitAB()\n",
        "mc_every_visit_ab.run_episodes(num_episodes)\n",
        "\n",
        "# Print the learned Q-values for states A and B:\n",
        "print(\"Learned Q-values for state A:\")\n",
        "for action in mc_every_visit_ab.actions:\n",
        "    print(f\"Q(A, {action}): {mc_every_visit_ab.q_values[('A', action)]}\")\n",
        "\n",
        "print(\"Learned Q-values for state B:\")\n",
        "for action in mc_every_visit_ab.actions:\n",
        "    print(f\"Q(B, {action}): {mc_every_visit_ab.q_values[('B', action)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ_svr3xBOLu",
        "outputId": "a27815c1-06ba-4240-a792-ee2bbd38fc7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values for state A:\n",
            "Q(A, 0): 2.6871295942929816\n",
            "Q(A, 1): 1.4998987904887817\n",
            "Learned Q-values for state B:\n",
            "Q(B, 0): 0.8240793591459981\n",
            "Q(B, 1): 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def monte_carlo_first_visit(env, policy, num_episodes):\n",
        "    \"\"\"\n",
        "    Implements the Monte Carlo first visit method.\n",
        "\n",
        "    Args:\n",
        "        env: The environment.\n",
        "        policy: The policy.\n",
        "        num_episodes: The number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        The state-value function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the state-value function.\n",
        "    state_value = np.zeros(env.n_states)\n",
        "\n",
        "    # Run the Monte Carlo algorithm for the specified number of episodes.\n",
        "    for _ in range(num_episodes):\n",
        "        # Generate an episode.\n",
        "        episode = generate_episode(env, policy)\n",
        "\n",
        "        # Keep track of visited states in this episode.\n",
        "        visited_states = set()\n",
        "\n",
        "        # Update the state-value function for each state visited in the episode (first visit).\n",
        "        for t in range(len(episode)):\n",
        "            state, reward, _, _ = episode[t]\n",
        "\n",
        "            # Check if it's the first visit to the state in this episode.\n",
        "            if state not in visited_states:\n",
        "                visited_states.add(state)\n",
        "                state_value[state] = (state_value[state] * state_value[state] + reward) / (state_value[state] + 1)\n",
        "\n",
        "    print(\"Learned State-Value Function (Monte Carlo First Visit):\", state_value)\n",
        "    return state_value\n",
        "\n",
        "def monte_carlo_every_visit(env, policy, num_episodes):\n",
        "    \"\"\"\n",
        "    Implements the Monte Carlo every visit method.\n",
        "\n",
        "    Args:\n",
        "        env: The environment.\n",
        "        policy: The policy.\n",
        "        num_episodes: The number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        The state-value function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the state-value function.\n",
        "    state_value = np.zeros(env.n_states)\n",
        "\n",
        "    # Run the Monte Carlo algorithm for the specified number of episodes.\n",
        "    for _ in range(num_episodes):\n",
        "        # Generate an episode.\n",
        "        episode = generate_episode(env, policy)\n",
        "\n",
        "        # Update the state-value function for each state visited in the episode (every visit).\n",
        "        for t in range(len(episode)):\n",
        "            state, reward, _, _ = episode[t]\n",
        "            state_value[state] += reward\n",
        "\n",
        "    print(\"Learned State-Value Function (Monte Carlo Every Visit):\", state_value)\n",
        "    return state_value\n",
        "\n",
        "def generate_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Generates an episode.\n",
        "\n",
        "    Args:\n",
        "        env: The environment.\n",
        "        policy: The policy.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples representing the episode.\n",
        "    \"\"\"\n",
        "\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode.append((state, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "    return episode\n",
        "\n",
        "# Example usage with a dummy environment and policy function\n",
        "class DummyEnv:\n",
        "    def __init__(self):\n",
        "        self.n_states = 3\n",
        "\n",
        "    def reset(self):\n",
        "        return 0\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            return 1, 1.0, False, {}\n",
        "        elif action == 1:\n",
        "            return 2, 0.5, False, {}\n",
        "\n",
        "# Dummy policy that always returns action 0\n",
        "def dummy_policy(state):\n",
        "    return 0\n",
        "\n",
        "# Example usage:\n",
        "num_episodes = 1000\n",
        "\n",
        "# First Visit\n",
        "monte_carlo_first_visit_result = monte_carlo_first_visit(DummyEnv(), dummy_policy, num_episodes)\n",
        "\n",
        "# Every Visit\n",
        "monte_carlo_every_visit_result = monte_carlo_every_visit(DummyEnv(), dummy_policy, num_episodes)\n"
      ],
      "metadata": {
        "id": "lkUYcbOyBpfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}